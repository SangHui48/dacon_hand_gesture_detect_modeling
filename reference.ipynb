{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-09-10T06:26:39.485853Z","iopub.status.busy":"2022-09-10T06:26:39.485341Z","iopub.status.idle":"2022-09-10T06:26:59.351122Z","shell.execute_reply":"2022-09-10T06:26:59.349394Z","shell.execute_reply.started":"2022-09-10T06:26:39.485811Z"},"trusted":true},"outputs":[],"source":["!pip install -q git+https://github.com/tensorflow/docs"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Data Collection"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-10T06:27:45.194345Z","iopub.status.busy":"2022-09-10T06:27:45.193095Z","iopub.status.idle":"2022-09-10T06:28:02.778394Z","shell.execute_reply":"2022-09-10T06:28:02.776669Z","shell.execute_reply.started":"2022-09-10T06:27:45.194291Z"},"trusted":true},"outputs":[],"source":["!wget -q https://git.io/JGc31 -O ucf101_top5.tar.gz\n","!tar xf ucf101_top5.tar.gz"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-10T06:31:12.644993Z","iopub.status.busy":"2022-09-10T06:31:12.644486Z","iopub.status.idle":"2022-09-10T06:31:12.95231Z","shell.execute_reply":"2022-09-10T06:31:12.951007Z","shell.execute_reply.started":"2022-09-10T06:31:12.644945Z"},"trusted":true},"outputs":[],"source":["from tensorflow_docs.vis import embed\n","from tensorflow import keras\n","from imutils import paths\n","\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import imageio\n","import cv2\n","import os"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Define hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-10T06:31:35.635355Z","iopub.status.busy":"2022-09-10T06:31:35.634936Z","iopub.status.idle":"2022-09-10T06:31:35.641746Z","shell.execute_reply":"2022-09-10T06:31:35.640818Z","shell.execute_reply.started":"2022-09-10T06:31:35.635322Z"},"trusted":true},"outputs":[],"source":["IMG_SIZE = 224\n","BATCH_SIZE = 64\n","EPOCHS = 10\n","\n","MAX_SEQ_LENGTH = 20\n","NUM_FEATURES = 2048"]},{"cell_type":"markdown","metadata":{},"source":["## 4. Data preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-10T06:31:49.675625Z","iopub.status.busy":"2022-09-10T06:31:49.675119Z","iopub.status.idle":"2022-09-10T06:31:49.740505Z","shell.execute_reply":"2022-09-10T06:31:49.738961Z","shell.execute_reply.started":"2022-09-10T06:31:49.675582Z"},"trusted":true},"outputs":[],"source":["train_df = pd.read_csv(\"train.csv\")\n","test_df = pd.read_csv(\"test.csv\")\n","\n","print(f\"Total videos for training: {len(train_df)}\")\n","print(f\"Total videos for testing: {len(test_df)}\")\n","\n","train_df.sample(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-10T06:32:32.356257Z","iopub.status.busy":"2022-09-10T06:32:32.354817Z","iopub.status.idle":"2022-09-10T06:32:32.366939Z","shell.execute_reply":"2022-09-10T06:32:32.365966Z","shell.execute_reply.started":"2022-09-10T06:32:32.356186Z"},"trusted":true},"outputs":[],"source":["def crop_center_square(frame):\n","    y, x = frame.shape[0:2]\n","    min_dim = min(y, x)\n","    start_x = (x // 2) - (min_dim // 2)\n","    start_y = (y // 2) - (min_dim // 2)\n","    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n","\n","\n","def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n","    cap = cv2.VideoCapture(path)\n","    frames = []\n","    try:\n","        while True:\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            frame = crop_center_square(frame)\n","            frame = cv2.resize(frame, resize)\n","            frame = frame[:, :, [2, 1, 0]]\n","            frames.append(frame)\n","\n","            if len(frames) == max_frames:\n","                break\n","    finally:\n","        cap.release()\n","    return np.array(frames)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-10T06:32:47.787502Z","iopub.status.busy":"2022-09-10T06:32:47.786961Z","iopub.status.idle":"2022-09-10T06:32:53.535284Z","shell.execute_reply":"2022-09-10T06:32:53.533945Z","shell.execute_reply.started":"2022-09-10T06:32:47.787461Z"},"trusted":true},"outputs":[],"source":["def build_feature_extractor():\n","    feature_extractor = keras.applications.InceptionV3(\n","        weights=\"imagenet\",\n","        include_top=False,\n","        pooling=\"avg\",\n","        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n","    )\n","    preprocess_input = keras.applications.inception_v3.preprocess_input\n","\n","    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n","    preprocessed = preprocess_input(inputs)\n","\n","    outputs = feature_extractor(preprocessed)\n","    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n","\n","\n","feature_extractor = build_feature_extractor()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-10T06:33:08.756531Z","iopub.status.busy":"2022-09-10T06:33:08.755426Z","iopub.status.idle":"2022-09-10T06:33:08.814663Z","shell.execute_reply":"2022-09-10T06:33:08.813345Z","shell.execute_reply.started":"2022-09-10T06:33:08.756479Z"},"trusted":true},"outputs":[],"source":["label_processor = keras.layers.StringLookup(\n","    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"])\n",")\n","print(label_processor.get_vocabulary())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-10T06:33:44.905179Z","iopub.status.busy":"2022-09-10T06:33:44.904612Z","iopub.status.idle":"2022-09-10T07:15:42.457339Z","shell.execute_reply":"2022-09-10T07:15:42.456288Z","shell.execute_reply.started":"2022-09-10T06:33:44.905136Z"},"trusted":true},"outputs":[],"source":["def prepare_all_videos(df, root_dir):\n","    num_samples = len(df)\n","    video_paths = df[\"video_name\"].values.tolist()\n","    labels = df[\"tag\"].values\n","    labels = label_processor(labels[..., None]).numpy()\n","\n","    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n","    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n","    # masked with padding or not.\n","    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n","    frame_features = np.zeros(\n","        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n","    )\n","\n","    # For each video.\n","    for idx, path in enumerate(video_paths):\n","        # Gather all its frames and add a batch dimension.\n","        frames = load_video(os.path.join(root_dir, path))\n","        frames = frames[None, ...]\n","\n","        # Initialize placeholders to store the masks and features of the current video.\n","        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n","        temp_frame_features = np.zeros(\n","            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n","        )\n","\n","        # Extract features from the frames of the current video.\n","        for i, batch in enumerate(frames):\n","            video_length = batch.shape[0]\n","            length = min(MAX_SEQ_LENGTH, video_length)\n","            for j in range(length):\n","                temp_frame_features[i, j, :] = feature_extractor.predict(\n","                    batch[None, j, :]\n","                )\n","            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n","\n","        frame_features[idx,] = temp_frame_features.squeeze()\n","        frame_masks[idx,] = temp_frame_mask.squeeze()\n","\n","    return (frame_features, frame_masks), labels\n","\n","\n","train_data, train_labels = prepare_all_videos(train_df, \"train\")\n","test_data, test_labels = prepare_all_videos(test_df, \"test\")\n","\n","print(f\"Frame features in train set: {train_data[0].shape}\")\n","print(f\"Frame masks in train set: {train_data[1].shape}\")"]},{"cell_type":"markdown","metadata":{},"source":["## 5. The sequence model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-10T07:19:24.945466Z","iopub.status.busy":"2022-09-10T07:19:24.944979Z","iopub.status.idle":"2022-09-10T07:19:48.682536Z","shell.execute_reply":"2022-09-10T07:19:48.681145Z","shell.execute_reply.started":"2022-09-10T07:19:24.945428Z"},"trusted":true},"outputs":[],"source":["def get_sequence_model():\n","    class_vocab = label_processor.get_vocabulary()\n","\n","    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n","    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n","\n","    # Refer to the following tutorial to understand the significance of using `mask`:\n","    # https://keras.io/api/layers/recurrent_layers/gru/\n","    x = keras.layers.GRU(16, return_sequences=True)(\n","        frame_features_input, mask=mask_input\n","    )\n","    x = keras.layers.GRU(8)(x)\n","    x = keras.layers.Dropout(0.4)(x)\n","    x = keras.layers.Dense(8, activation=\"relu\")(x)\n","    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n","\n","    rnn_model = keras.Model([frame_features_input, mask_input], output)\n","\n","    rnn_model.compile(\n","        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n","    )\n","    return rnn_model\n","\n","\n","# Utility for running experiments.\n","def run_experiment():\n","    filepath = \"/tmp/video_classifier\"\n","    checkpoint = keras.callbacks.ModelCheckpoint(\n","        filepath, save_weights_only=True, save_best_only=True, verbose=1\n","    )\n","\n","    seq_model = get_sequence_model()\n","    history = seq_model.fit(\n","        [train_data[0], train_data[1]],\n","        train_labels,\n","        validation_split=0.3,\n","        epochs=EPOCHS,\n","        callbacks=[checkpoint],\n","    )\n","\n","    seq_model.load_weights(filepath)\n","    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n","    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n","\n","    return history, seq_model\n","\n","\n","_, sequence_model = run_experiment()"]},{"cell_type":"markdown","metadata":{},"source":["## 6. Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-10T07:19:49.701674Z","iopub.status.busy":"2022-09-10T07:19:49.701283Z","iopub.status.idle":"2022-09-10T07:19:56.001426Z","shell.execute_reply":"2022-09-10T07:19:55.999997Z","shell.execute_reply.started":"2022-09-10T07:19:49.70164Z"},"trusted":true},"outputs":[],"source":["def prepare_single_video(frames):\n","    frames = frames[None, ...]\n","    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n","    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n","\n","    for i, batch in enumerate(frames):\n","        video_length = batch.shape[0]\n","        length = min(MAX_SEQ_LENGTH, video_length)\n","        for j in range(length):\n","            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n","        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n","\n","    return frame_features, frame_mask\n","\n","\n","def sequence_prediction(path):\n","    class_vocab = label_processor.get_vocabulary()\n","\n","    frames = load_video(os.path.join(\"test\", path))\n","    frame_features, frame_mask = prepare_single_video(frames)\n","    probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n","\n","    for i in np.argsort(probabilities)[::-1]:\n","        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n","    return frames\n","\n","\n","# This utility is for visualization.\n","# Referenced from:\n","# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n","def to_gif(images):\n","    converted_images = images.astype(np.uint8)\n","    imageio.mimsave(\"animation.gif\", converted_images, fps=10)\n","    return embed.embed_file(\"animation.gif\")\n","\n","\n","test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\n","print(f\"Test video path: {test_video}\")\n","test_frames = sequence_prediction(test_video)\n","to_gif(test_frames[:MAX_SEQ_LENGTH])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.9 (default, May 17 2022, 12:55:41) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":4}
